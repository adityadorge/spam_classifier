{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import urllib\n",
    "import tarfile\n",
    "# import quopri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urllib and tarfile are pre-installed in python > urllib used to for files/website related operations & tarfile is used for zip file related operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from: https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
      "Saving to: /home/t460/Documents/ollama/datasets/spam/20021010_easy_ham.tar.bz2\n",
      "Downloading from: https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
      "Saving to: /home/t460/Documents/ollama/datasets/spam/20021010_spam.tar.bz2\n",
      "/home/t460/Documents/ollama/datasets/spam/easy_ham\n",
      "/home/t460/Documents/ollama/datasets/spam/spam\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "def fetch_data(base_url, files,download_path):\n",
    "    for file in files:\n",
    "        # Construct the full URL\n",
    "        file_url = f\"{base_url}{file}\"\n",
    "        file_download_path = os.path.join(download_path, file)\n",
    "        print(f\"Downloading from: {file_url}\")\n",
    "        print(f\"Saving to: {file_download_path}\")\n",
    "\n",
    "        # Download and save the file\n",
    "        # try:\n",
    "        #     # Download the file\n",
    "        #     urllib.request.urlretrieve(file_url, file_download_path)\n",
    "        #     print(f\"File successfully downloaded and saved as {file_download_path}\")\n",
    "\n",
    "        #     # Verify the file is a valid .tar.bz2 and extract it\n",
    "        #     with tarfile.open(file_download_path, \"r:bz2\") as tar:\n",
    "        #         tar.extractall(path=download_path)\n",
    "        #         print(f\"Files successfully extracted to {download_path}\")\n",
    "        # except tarfile.TarError as e:\n",
    "        #     print(f\"TarError while extracting {file}: {e}\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return [os.path.join(download_path,dir_name) for dir_name in (\"easy_ham\", \"spam\")] \n",
    "\n",
    "# Define the base URL, file names, and download path\n",
    "base_url = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "download_path = \"/home/t460/Documents/ollama/datasets/spam/\" # (absoulte path) Instead can use !from pathlib import Path\n",
    "files = [\"20021010_easy_ham.tar.bz2\", \"20021010_spam.tar.bz2\"]\n",
    "\n",
    "# Ensure the download directory exists\n",
    "# os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "# Fetch and extract the data\n",
    "ham_dir , spam_dir = fetch_data(base_url, files, download_path)\n",
    "print(ham_dir)\n",
    "print(spam_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the structure of the email. Creating dataset which consist of filtered hams&spams to feed to the model.<br>\n",
    "The dataset should consist of 4 :\n",
    "- sender's email and other important fields\n",
    "- subject\n",
    "- content of the email\n",
    "- and a column stating is it spam or ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email contains HTML content or is a plain-text email, you can inspect the MIME type of its body parts. This can be done using Python's email module.\n",
    "Emails often have MIME types like:\n",
    "- text/plain for plain-text emails.\n",
    "- text/html for HTML emails.\n",
    "\n",
    "If an email has both text/plain and text/html, it's a multipart email where one part is plain text (for compatibility) and another is HTML (for richer formatting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.policy import default\n",
    "from email.parser import BytesParser\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to extract email content (plain text or fallback to HTML)\n",
    "def get_email_content(email):\n",
    "    for part in email.walk():\n",
    "        html = None\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"): # if content_type is other than plain text or html than ignore\n",
    "            continue\n",
    "        try:\n",
    "            # get the character dataset for emails\n",
    "            charset = part.get_content_charset() or \"utf-8\"  # Default to UTF-8\n",
    "            # extract the content with respect to that charaset else throws error of \"string argument should contain only ASCII characters\"\n",
    "            content = part.get_payload(decode=True).decode(charset, errors=\"replace\")\n",
    "        except Exception as e:\n",
    "            content = part.get_payload(decode=True).decode(\"utf-8\", errors=\"replace\")  # Fallback\n",
    "        if ctype == \"text/plain\":\n",
    "            return content.strip()\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        soup = BeautifulSoup(html, 'html.parser') #convert to beautifulsoup object\n",
    "        decoded_html_content = soup.get_text(separator=\"\\n\", strip=True) # extract the content from html\n",
    "        return decoded_html_content\n",
    "    \n",
    "# Function to parse email and extract fields\n",
    "def parse_email(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            email = BytesParser(policy=default).parse(f)\n",
    "        \n",
    "        # Extract fields\n",
    "        email_data = {\n",
    "            #\"Receiver\": msg.get(\"Delivered-To\"),\n",
    "            \"From\": email.get(\"From\"),\n",
    "            #\"To\": msg.get(\"To\"),\n",
    "            \"Subject\": email.get(\"Subject\"),\n",
    "            \"Content\": get_email_content(email),\n",
    "        }\n",
    "        return email_data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          From  \\\n",
      "0        Chris Kloiber <ckloiber@ckloiber.com>   \n",
      "1      Dermot Daly <dermot.daly@itsmobile.com>   \n",
      "2             Owen Byrne <owen@permafrost.net>   \n",
      "3              Glen Gray <glen@netnoteinc.com>   \n",
      "4  Eirikur Hallgrimsson <eh@mad.scientist.com>   \n",
      "\n",
      "                                             Subject  \\\n",
      "0                      Re: RH 8 no DMA for DVD drive   \n",
      "1                 [ILUG] What HOWTOs for SOHO system   \n",
      "2                              Re: The case for spam   \n",
      "3  [ILUG] Retrieving read mail from webmail.eirco...   \n",
      "4                              process music: Mekons   \n",
      "\n",
      "                                             Content Label  \n",
      "0  On Mon, 2002-10-07 at 13:28, Matthias Saou wro...   ham  \n",
      "1  Hi All,\\nI'm trying to set up the following:\\n...   ham  \n",
      "2  Bill Stoddard wrote:\\n\\n>>No one likes commerc...   ham  \n",
      "3  Is there a way to get my read email downloaded...   ham  \n",
      "4  http://reuters.com/news_article.jhtml?type=ent...   ham  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load emails and extract fields\n",
    "def process_email_directory(directory):\n",
    "    emails = []\n",
    "    for file_path in directory.iterdir():\n",
    "        if file_path.is_file():\n",
    "            email_data = parse_email(file_path)\n",
    "            if email_data:\n",
    "                emails.append(email_data)\n",
    "    return emails\n",
    "\n",
    "# Path to email directories\n",
    "ham_dir = Path(ham_dir)\n",
    "spam_dir = Path(spam_dir)\n",
    "\n",
    "# Process ham and spam directories\n",
    "ham_emails = process_email_directory(ham_dir)\n",
    "spam_emails = process_email_directory(spam_dir)\n",
    "\n",
    "#______________________________________________________XXXXXX_________________________________________________________\n",
    "\n",
    "# Combine ham and spam emails into a single dataset\n",
    "email_data = pd.DataFrame(ham_emails + spam_emails)\n",
    "\n",
    "\n",
    "# Add labels for classification\n",
    "email_data[\"Label\"] = [\"ham\"] * len(ham_emails) + [\"spam\"] * len(spam_emails)\n",
    "\n",
    "# Save to CSV for model training\n",
    "# email_data.to_csv(\"email_dataset.csv\", index=False)\n",
    "\n",
    "print(email_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing Values with the most frequent values of each columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tim Chapman <timc@2ubh.com>\n",
      "Re: Java is for kiddies\n",
      "CONSANTLY\n",
      "being\n",
      "bombarded by so-called FREE money-making systems that teases you with limited\n",
      "information, and when its all said and done, blind-sides you by demanding your\n",
      "money/credit card information upfront in some slick way,\n",
      "after-the-fact\n",
      "!\n",
      "Yes, I too was as skeptical about such offers and the Internet in general with\n",
      "all its hype, as you probably are. Fortunate for me, my main business\n",
      "slowed-down (\n",
      "I have been self-employed all my life\n",
      "), so I looked for\n",
      "something to fit my lifestyle and some other way to assist me in paying my\n",
      "bills, without working myself to death or loosing more money; then, this\n",
      "proposal to try something new without any upfront investment (\n",
      "great! because\n",
      "I had none\n",
      ") interested me to click on the link provided. And I dont regret\n",
      "at all that I did! I am very happy, and happy enough to recommend it to you as\n",
      "a system that is true to its word. I mean absolutely no upfront money. You join\n",
      "only if (\n",
      "when\n",
      ") you make money. You also get to track the results of your\n",
      "time and efforts instantly and updated daily! I especially liked this idea of\n",
      "personal control with real-time, staying informed statistics.\n",
      "This system is quite simply\n",
      "the most logical, opened, and fair of any others that Ive seen before. Why?\n",
      "Because from the start, you get all the specific facts you need to seriously\n",
      "consider if this is right for you.  No teasing. No grand testimonies! No\n",
      "kidding! Just the facts! Unlike in other programs that give you no idea of\n",
      "their overall plan before first forking over your money/credit card; or worst\n",
      "yet, joining and finding-out too late, after wasting valuable time trying to\n",
      "figure them out, this system is straightforward and informative, providing you\n",
      "with the two things you really must know: \n",
      "Whats it all about\n",
      "? and \n",
      "How\n",
      "does it work\n",
      "?. These are the ultimate deal makers or deal breakers that\n",
      "need to be immediately disclosed, well before discovering that maybe you dont\n",
      "want to do that; by then you are hooked and now locked into a frustrating\n",
      "battle to try to get your money back!\n",
      "I call this my Platinum\n",
      "Choice because it stands alone as a true, superior deal that is totally\n",
      "different from previously misleading, hook-first programs that promise lofty\n",
      "mega-money jackpots, but really just want your money upfront to line their own\n",
      "pockets! Youve seen the headlines: \n",
      "Join free and Make $10,000 every week\n",
      "for life\n",
      " yeah, right!\n",
      "I did not make millions yet,\n",
      "but the whole thing was launched just a few weeks ago and I am more than happy\n",
      "with my earnings, so far. I must tell you, I wouldnt be able to do anything\n",
      "without corporate help  which was unusually thorough, timely, and motivating.\n",
      "You have to see this in action\n",
      "for yourself and make up your own mind; just go to my site and fill out the\n",
      "form as soon as you can. You will get your own site in a few minutes. Then you\n",
      "are ready to try whether you can make some decent money with this system and\n",
      "the Internets explosive potential - fully loaded with hi-tech software, free\n",
      "corporate help, on-time members support and even protective safeguards!\n",
      "Get it now, and you can call me\n",
      "at any time with questions. It really could help you like it is helping me to\n",
      "finally be able to pay my bills, and keep my free time free.  Good luck!\n",
      "http://www.mindupmerchants.com/default.asp?ID=5581\n",
      "Ben Green, (775) 322-3323\n",
      "P.S.Free POP3 email is ofered for members now!\n",
      "ham\n"
     ]
    }
   ],
   "source": [
    "# check the most frequent values of each columns \n",
    "# for column in email_data.columns:\n",
    "#     print(email_data[column].mode()[0])\n",
    "\n",
    "# Replacing missing values with the most common values of each column\n",
    "for column in email_data.columns:\n",
    "    email_data[column].fillna(email_data[column].mode()[0], inplace=True)  \n",
    "\n",
    "# # Saving the updated DataFrame to a CSV file\n",
    "email_data.to_csv(\"email_dataset2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(804, 3)\n"
     ]
    }
   ],
   "source": [
    "X = email_data.drop('Label',axis=1)\n",
    "y = email_data['Label'] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test =  train_test_split(X,y,test_size=0.2,random_state=42,shuffle=True)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing step :\n",
    "- <span style=\"color:orange\"> Tokenization: </span> Split text into words or subwords.\n",
    "- <span style=\"color:orange\"> Normalization: </span> Lowercase, remove punctuation, etc.\n",
    "- <span style=\"color:orange\"> word2vec: </span> convert the text to numerical representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.base import BaseEstimator , TransformerMixin\n",
    "\n",
    "# class CustomTransformer(BaseEstimator , TransformerMixin):\n",
    "#     pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
